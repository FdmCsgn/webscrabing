# ğŸŒ Web KazÄ±ma (Web Scraping) Projesi

Beautiful Soup kullanarak web sayfalarÄ±ndan veri Ã§Ä±karma ve toplama rehberi.

## ğŸ“š Beautiful Soup Nedir?

Beautiful Soup, Python'da web sayfalarÄ±nÄ± parse etmek ve veri Ã§Ä±karmak iÃ§in kullanÄ±lan gÃ¼Ã§lÃ¼ bir kÃ¼tÃ¼phanedir. HTML ve XML dosyalarÄ±nÄ± parse ederek iÃ§erilerinden istediÄŸimiz verileri Ã§Ä±karmamÄ±zÄ± saÄŸlar.

## ğŸ› ï¸ Temel Kurulum

```python
# Gerekli kÃ¼tÃ¼phanelerin kurulumu
pip install beautifulsoup4
pip install requests
pip install lxml

# Temel importlar
from bs4 import BeautifulSoup
import requests
import os
import time
```

## ğŸ” Temel KullanÄ±m Ã–rnekleri

### 1. Web SayfasÄ±nÄ± Ã‡ekme
```python
# Web sayfasÄ±na istek atma
url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
```

### 2. Metin Verisi Ã‡Ä±karma
```python
# BaÅŸlÄ±klarÄ± Ã§ekme
titles = soup.find_all('h1')
for title in titles:
    print(title.text.strip())

# ParagraflarÄ± Ã§ekme
paragraphs = soup.find_all('p')
for p in paragraphs:
    print(p.text.strip())
```

### 3. GÃ¶rsel Ã‡Ä±karma
```python
# Resimleri Ã§ekme ve kaydetme
images = soup.find_all('img')
for img in images:
    img_url = img.get('src')
    if img_url:
        img_data = requests.get(img_url).content
        with open(f'images/{img_url.split("/")[-1]}', 'wb') as f:
            f.write(img_data)
```

## ğŸ¯ Veri Ã‡Ä±karma Teknikleri

### 1. CSS SeÃ§iciler
```python
# Class ile seÃ§me
content = soup.find('div', class_='content')

# ID ile seÃ§me
header = soup.find('div', id='header')

# Multiple seÃ§iciler
links = soup.select('div.content a')
```

### 2. Ã–zel Filtreler
```python
# Lambda fonksiyonlarÄ± ile filtreleme
specific_divs = soup.find_all('div', attrs={
    'class': lambda x: x and 'specific-class' in x
})
```

## ğŸ›¡ï¸ Best Practices ve Dikkat Edilmesi Gerekenler

1. **Rate Limiting**
```python
# Ä°stekler arasÄ±na gecikme ekleme
import time

def scrape_with_delay(urls, delay=2):
    for url in urls:
        response = requests.get(url)
        # Ä°ÅŸlemler...
        time.sleep(delay)  # Saniye cinsinden bekleme
```

2. **Hata YÃ¶netimi**
```python
try:
    response = requests.get(url, timeout=5)
    response.raise_for_status()
except requests.RequestException as e:
    print(f"Hata oluÅŸtu: {e}")
```

3. **User-Agent Belirtme**
```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
response = requests.get(url, headers=headers)
```

## ğŸ“‚ Veri Saklama Stratejileri

### 1. Dosya Sistemi
```python
# Metin verilerini kaydetme
with open('data.txt', 'w', encoding='utf-8') as f:
    f.write(extracted_text)

# GÃ¶rselleri kaydetme
def save_image(img_url, folder='images'):
    if not os.path.exists(folder):
        os.makedirs(folder)
    response = requests.get(img_url)
    file_name = os.path.join(folder, img_url.split('/')[-1])
    with open(file_name, 'wb') as f:
        f.write(response.content)
```

### 2. VeritabanÄ± Entegrasyonu
```python
import sqlite3

def save_to_db(data):
    conn = sqlite3.connect('scraping.db')
    c = conn.cursor()
    c.execute('''INSERT INTO scraped_data 
                 (title, content, url) 
                 VALUES (?, ?, ?)''', 
                 (data['title'], data['content'], data['url']))
    conn.commit()
    conn.close()
```

## ğŸ”„ Dinamik Ä°Ã§erik Ä°ÅŸleme

### 1. JavaScript Ä°Ã§eriÄŸi
```python
from selenium import webdriver

def get_dynamic_content(url):
    driver = webdriver.Chrome()
    driver.get(url)
    # SayfanÄ±n yÃ¼klenmesini bekle
    time.sleep(2)
    page_source = driver.page_source
    driver.quit()
    return BeautifulSoup(page_source, 'html.parser')
```

## ğŸ“Š Veri Temizleme ve Ä°ÅŸleme

```python
def clean_text(text):
    # Gereksiz boÅŸluklarÄ± temizle
    text = ' '.join(text.split())
    # HTML karakterlerini Ã§Ã¶z
    text = BeautifulSoup(text, 'html.parser').get_text()
    return text.strip()
```

## ğŸš« Etik ve Yasal Konular

1. `robots.txt` dosyasÄ±na uyma
2. Rate limiting uygulama
3. Telif haklarÄ±na saygÄ± gÃ¶sterme
4. KiÅŸisel verileri koruma
5. Site kullanÄ±m ÅŸartlarÄ±na uyma

## ğŸ” Performans Optimizasyonu

1. **Asenkron Ä°stekler**
```python
import asyncio
import aiohttp

async def fetch_page(session, url):
    async with session.get(url) as response:
        return await response.text()

async def scrape_multiple_pages(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_page(session, url) for url in urls]
        return await asyncio.gather(*tasks)
```

2. **Multiprocessing**
```python
from multiprocessing import Pool

def scrape_url(url):
    return requests.get(url).text

with Pool(4) as p:
    results = p.map(scrape_url, urls)
```
